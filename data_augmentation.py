# -*- coding: utf-8 -*-
"""DATA  AUGMENTATION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xl4HeAjDQfYZspz677oqM1cf0xwZyGh7
"""

# Commented out IPython magic to ensure Python compatibility.
# # Note: restart runtime after this import before running the augmentations
# %%capture
# from IPython.display import clear_output
# !pip install tdqm split-folders[full] opencv-python
# !pip matplotlib
# clear_output()

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import collections
# import sys
# import os
# from google.colab import drive
# import glob
# from tqdm import tqdm
# import shutil
# import splitfolders
# import cv2
# from multiprocessing import Pool
# from multiprocessing import cpu_count
# import numpy as np
# import os
# from pathlib import Path
# import matplotlib.pyplot as plt
# import multiprocessing

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# drive.mount('/content/drive')

root_folder = '/content/drive/MyDrive/KSL/'
tagged_videos = '/content/drive/MyDrive/KSL/TaggedVideo/'
augments = '/content/drive/MyDrive/KSL/Augments/'
files= glob.glob(tagged_videos + "*")
yolo_dir = '/content/drive/MyDrive/KSL/Yolo/'
tagged_videos0 = '/content/drive/MyDrive/KSL/TaggedVideo0/'

classes_dir = "/content/drive/MyDrive/KSL/Classes/"
subsets_dir = "/content/drive/MyDrive/KSL/Subsets/"
segments_dir = "/content/drive/MyDrive/KSL/Segments/"
subsets_paths ={
    'train':subsets_dir +'train/',
    'val':subsets_dir +'val/',
    'test':subsets_dir +'test/'
}

file_dirs = [classes_dir,subsets_dir, augments]
for file_dir in file_dirs:
  if not os.path.exists(file_dir):
    os.makedirs(file_dir)

for key in subsets_paths:
   if not os.path.exists(subsets_paths[key]):
    os.makedirs(subsets_paths[key])

global net
net = cv2.dnn.readNet(f"{yolo_dir}yolov3.weights", f"{yolo_dir}yolov3.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

def readFrames(video_path):
      cap = cv2.VideoCapture(video_path)
      frames = []
      while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
      cap.release()
      print(f'Len:: {len(frames)}')
      return frames

def frames2video(frames,video):
    try:
      height, width, layers = frames[0].shape
      print(f'Height: {height}, Width: {width}, Layers: {layers}')
      output_filename = f"{os.path.basename(video)}"
      video_path = os.path.join(tagged_videos0, output_filename)
      size = (width, height)
      out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, size)
      # out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'avc1'), 30, size)
      for frame in frames:
        out.write(frame)
      out.release()
      # print('Video Save Done', video_path)
      # frame = frames[4]
      # plt.imshow(frame)
      # plt.show()
    except Exception as e:
      print(e)

def humanSegment(frame):
      blob = cv2.dnn.blobFromImage(
      frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
      net.setInput(blob)
      outs = net.forward(output_layers)
      conf_threshold = 0.5
      class_id = 0
      boxes = []

      for out in outs:
        for detection in out:
          scores = detection[5:]
          if scores[class_id] > conf_threshold:
            center_x, center_y, w, h = (detection[:4] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])).astype(int)
            x, y = center_x - w // 2, center_y - h // 2
            boxes.append((x, y, w, h))

      # Extract ROIs from the frame using Numpy
      segmented_frame = np.zeros_like(frame)
      for (x, y, w, h) in boxes:
        segmented_frame[y:y+h, x:x+w] = frame[y:y+h, x:x+w]
      return segmented_frame

def segmentVideo(video_path):
  frames = readFrames(video_path)
  frames = [humanSegment(frame) for frame in frames]
  frame = frames[3]
  # show image matplotlib
  # plt.imshow(frame)
  # plt.show()
  height, width, layers = frame.shape
  print(f'Height: {height}, Width: {width}, Layers: {layers}')
  # save video
  frames2video(frames, video_path)

def processFile(file_path):
  segmentVideo(file_path)

def process_vid(video,output_directory):
    frames = readFrames(video)
    print(len(frames))
    frame_height, frame_width, _ = frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    output_filename = f"{os.path.basename(video)}"
    output_path = os.path.join(output_directory, output_filename)
    out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))
    for frame in frames:
      out.write(frame)
    out.release()

def process_vid_wrapper(file):
    process_vid(file, tagged_videos0)

def app(files):
    # Number of cores
    num_cores = 6

    # Creating a pool of processes
    with multiprocessing.Pool(num_cores) as pool:
        # Process the files in parallel
        for _ in tqdm(pool.imap_unordered(process_vid_wrapper, files), total=len(files)):
          clear_output()
          pbar.update(1)
          pass

pbar = tqdm(total=len(files))
app(files)
# for f in files:
#   process_vid(f,tagged_videos0)

def frameHRFlip(frame):
  return cv2.flip(frame, 1)

def frameBlur(frame):
  return cv2.blur(frame, (2,2))

def frameCenterCrop (image, zoom_factor=2):
  new_width = int(image.shape[1] * zoom_factor)
  new_height = int(image.shape[0] * zoom_factor)
  center_x, center_y = image.shape[1] // 2, image.shape[0] // 2
  crop_x1 = max(center_x - new_width // 2, 0)
  crop_x2 = min(center_x + new_width // 2, image.shape[1])
  crop_y1 = max(center_y - new_height // 2, 0)
  crop_y2 = min(center_y + new_height // 2, image.shape[0])
  zoomed_image = image[crop_y1:crop_y2, crop_x1:crop_x2]
  zoomed_image = cv2.resize(zoomed_image, (new_width, new_height))
  return zoomed_image

def colorJitter(image, hue_factor=50, contrast_factor=1.5, brightness_factor=50):
  hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
  hsv[:, :, 0] = (hsv[:, :, 0] + hue_factor) % 180
  hsv[:, :, 1] = np.clip(hsv[:, :, 1] * contrast_factor, 0, 255)
  hsv[:, :, 2] = np.clip(hsv[:, :, 2] + brightness_factor, 0, 255)
  modified_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
  return modified_image

def invert_colors(image):
  inverted_image = cv2.bitwise_not(image)
  return inverted_image

def frameShine(frame, alpha = 1.5,beta = 10):
  return cv2.convertScaleAbs(frame, alpha=alpha, beta=beta)

def frameDim(frame, contrast = 1.,brightness = 0.7):
  return cv2.addWeighted(frame, contrast, frame, 0, brightness)

def videoCrop(frames):
    return [frameCenterCrop(frame) for frame in frames]

def videoBlur(frames):
    return [frameBlur(frame) for frame in frames]

def videoDim(frames):
    return [frameDim(frame) for frame in frames]

def videoShine(frames):
    return [frameShine(frame) for frame in frames]

def videoHRFlip(frames):
    return [frameHRFlip(frame) for frame in frames]

def videoJitter(frames):
  return [colorJitter(frame) for frame in frames]

def colorInvert(frames):
  return [invert_colors(frame) for frame in frames]

def process_video(video, augmentations, output_directory):
  frames = readFrames(video)
  for augmentation_name, augmentation_function in augmentations.items():
    augmented_frames = augmentation_function(frames)
    # Create a VideoWriter to save the augmented video
    frame_height, frame_width, _ = augmented_frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    output_filename = f"{augmentation_name}_{os.path.basename(video)}"
    output_path = os.path.join(output_directory, output_filename)
    out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))
    for frame in augmented_frames:
      out.write(frame)
    out.release()

# Make sure the output directory exists
os.makedirs(augments, exist_ok=True)
augmentations = {
    "crop": videoCrop,
    "blur": videoBlur,
    "dim": videoDim,
    "shine": videoShine,
    "flip": videoHRFlip,
    "color_invert": colorInvert,
    "jitter": videoJitter
}

videos = glob.glob(tagged_videos + "*")

import multiprocessing
from tqdm import tqdm

def process_video_wrapper(args):
    video, augmentations, augments = args
    process_video(video, augmentations, augments)

def main(augmentations,videos,augments):
    num_cores = multiprocessing.cpu_count()
    args_list = [(video, augmentations, augments) for video in videos]
    with multiprocessing.Pool(num_cores) as pool:
        for _ in tqdm(pool.imap_unordered(process_video_wrapper, args_list), total=len(videos), desc="Videos Processing"):
          clear_output()
          pass
        pool.close()
main(augmentations,videos,augments)


# for video in tqdm(videos, desc="Videos Processing"):
#   process_video(video, augmentations, augments)
#   clear_output() # clear the output to show the progress bar

#Create clasesses from folders
output_videos = glob.glob(augments + "*")
files_count = {}
classes = set()
for _video in output_videos:
  file_name = os.path.basename(_video)
  _parts0 = file_name.split("_")[-1]
  file_class= _parts0.split(".")[0]
  classes.add(file_class)
  files_count[file_class] = 0

print(classes)
print(files_count)
for _class in classes:
  _class_dir = classes_dir+_class
  if not os.path.exists(_class_dir):
    os.makedirs(_class_dir)

pbar = tqdm(total=len(output_videos))
print(len(output_videos))
destination_directory = augments

# Get a list of all files in the source directory
source_files = os.listdir(augments)
errors = []
files_count = {}
classes = set()

def copy_file(task):
    source_path, destination_path, file_class = task
    try:
        with open(source_path, 'rb') as src_file, open(destination_path, 'wb') as dest_file:
            dest_file.write(src_file.read())
        return file_class
    except Exception as e:
        return None

def process_files():
    source_files = os.listdir(augments)
    tasks = []
    pbar = tqdm(total=len(output_videos))
    for _file in source_files:
        file_name = os.path.basename(_file)
        file_class = file_name.split("_")[-1].split(".")[0]
        source_path = os.path.join(classes_dir + file_class, _file)
        destination_path = os.path.join(destination_directory, _file)
        if os.path.isfile(source_path):
            tasks.append((source_path, destination_path, file_class))
        clear_output()
        pbar.update(1)
    with multiprocessing.Pool() as pool:
        results = pool.imap_unordered(copy_file, tasks)

process_files()

print(errors)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # split classes data set into train test validation (0.7, 0.2, 0.1)
# splitfolders.ratio(classes_dir, output=subsets_dir,seed=1337, ratio=(.6, .2, .2), group_prefix=None, move=False)
# # default values