# -*- coding: utf-8 -*-
"""CNN+RNN NET

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jTGx4BYwXT7KOXAWFLvmkgniM5-pW5IV
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pafy moviepy
# from IPython.display import clear_output
# !sudo apt-get install python3-magic
# !pip install remotezip einops tqdm tensorflow_docs tensorflow
# !pip install opencv-python youtube-dl moviepy

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# import cv2
# import pafy
# import math
# import random
# import numpy as np
# import datetime as dt
# import tensorflow as tf
# from collections import deque
# import matplotlib.pyplot as plt
# import time
# import glob
# 
# from moviepy.editor import *
# %matplotlib inline
# 
# from google.colab import drive
# import multiprocessing
# from functools import partial

seed_constant = 27
np.random.seed(seed_constant)
random.seed(seed_constant)
tf.random.set_seed(seed_constant)



from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import TimeDistributed,Conv2D, MaxPooling2D, LSTM, Dropout, Dense, Flatten, Activation, BatchNormalization, GRU
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.callbacks import EarlyStopping

drive.mount('/content/drive',force_remount=True)

# Specify the height and width to which each video frame will be resized in our dataset.
IMAGE_HEIGHT , IMAGE_WIDTH = 150, 150
# Specify the number of frames of a video that will be fed to the model as one sequence.
SEQUENCE_LENGTH = 22
# Specify the directory containing the UCF50 dataset.
DATASET_DIR = "/content/drive/MyDrive/KSL/Classes/"
# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.
CLASSES_LIST = ["work","home","come","go"]
MODEL_DIR ="/content/drive/MyDrive/KSL/Models/"

def process_frame(frame):
    resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
    normalized_frame = resized_frame / 255.0
    return normalized_frame

def extractFrames(videoFile):
    video = cv2.VideoCapture(videoFile)
    frames = []
    framesCount = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    frameInterval = max(int(framesCount / SEQUENCE_LENGTH), 1)

    for frameCounter in range(SEQUENCE_LENGTH):
        video.set(cv2.CAP_PROP_POS_FRAMES, frameCounter * frameInterval)
        success, frame = video.read()
        if not success:
            break
        frames.append(frame)

    video.release()
    frames = [process_frame(frame) for frame in frames]
    return frames

def plotMetrics(history, metric1, metric2,plotName):
    metricValue1 = history.history[metric1]
    metricValue2 = history.history[metric2]
    epochs = range(len(metricValue1))
    plt.plot(epochs, metricValue1, 'blue', label = metric1)
    plt.plot(epochs, metricValue2, 'red', label = metric2)
    plt.title(str(plotName))
    plt.legend()

# # Create the dataset.
# t = time.process_time()
# features = []
# labels = []
# for classIndex, className in enumerate(CLASSES_LIST):
#   class_dir = os.path.join(DATASET_DIR, className)
#   filesList = os.listdir(class_dir)
#   print(f"Class: {className} : FileCount: {len(filesList)} ")
#   for file_name in filesList[:120]:
#     videoFile = os.path.join(class_dir, file_name)
#     frames = extractFrames(videoFile)
#     if len(frames) == SEQUENCE_LENGTH:
#       features.append(frames)
#       labels.append(CLASSES_LIST.index(className))
# features = np.asarray(features)
# labels = np.array(labels)
# elapsed_time = time.process_time() - t
# print("Time Taken: ")
# print(elapsed_time)

def process_video_file(task):
    classIndex, className, file_name = task
    videoFile = os.path.join(DATASET_DIR, className, file_name)
    frames = extractFrames(videoFile)  # This function needs to be defined
    if len(frames) == SEQUENCE_LENGTH:
        return (frames, CLASSES_LIST.index(className))
    return None

def create_dataset():
    tasks = []
    for classIndex, className in enumerate(CLASSES_LIST):
        class_dir = os.path.join(DATASET_DIR, className)
        filesList = os.listdir(class_dir)
        print(f"Class: {className} : FileCount: {len(filesList)} ")
        for file_name in filesList[:120]:
            tasks.append((classIndex, className, file_name))

    pool = multiprocessing.Pool()
    results = pool.imap_unordered(process_video_file, tasks)
    pool.close()
    pool.join()

    # Filter out None results and separate features and labels
    features, labels = zip(*[result for result in results if result is not None])

    return np.asarray(features), np.array(labels)

t = time.time()
features, labels = create_dataset()
elapsed_time = time.time() - t
print("Time Taken:", elapsed_time)

# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors
print(len(labels))
one_hot_encoded_labels = to_categorical(labels)

# Split the Data into Train ( 75% ) and Test Set ( 25% ).
features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.30, shuffle = True, random_state = seed_constant)
print(features_train.shape)

def Model():
   # We will use a Sequential model for model construction.
    model = Sequential()
    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))

    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    #model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Flatten()))
    # model.add(TimeDistributed(BatchNormalization()))
    model.add(GRU(32))

    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))

  ########################################################################################################################

  # Display the models summary.
    model.summary()

  # Return the constructed LRCN model.
    return model

model =  Model()

plot_model(model=model, to_file = 'structure.png', show_shapes = True, show_layer_names = True)

early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)

# # Compile the model and specify loss function, optimizer and metrics to the model.
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ["accuracy"])

# # Start training the model.
model_training_history = model.fit(x = features_train, y = labels_train, epochs = 100, batch_size = 1, shuffle = True, validation_split = 0.2, callbacks = [])

model_evaluation_history = model.evaluate(features_test, labels_test)

from datetime import datetime
now = str(datetime.now()).replace("-","").replace(" ","").replace(".",":")
modelName = MODEL_DIR +f'{SEQUENCE_LENGTH}_{IMAGE_WIDTH}_{len(CLASSES_LIST)}_{now}_model.h5'
model.save(modelName)

# Visualize the training and validation loss metrices.
plotMetrics(model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')

# Visualize the training and validation accuracy metrices.
plotMetrics(model_training_history , 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')

saveModel = tf.keras.models.load_model(modelName)

def predictAction(videoFile, SEQUENCE_LENGTH):
  video = cv2.VideoCapture(videoFile)
  width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
  height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))

  framesList = []
  predictedClassName = ''
  framesCount = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
  frameWindow = max(int(framesCount/SEQUENCE_LENGTH),1)

  for frameCounter in range(SEQUENCE_LENGTH):
    video.set(cv2.CAP_PROP_POS_FRAMES, frameCounter * frameWindow)
    ret, frame = video.read()
    if not ret:
      break
    frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
    frame = frame / 255.0
    framesList.append(frame)

  predictedLabelsProbabilities = saveModel.predict(np.expand_dims(framesList, axis = 0))[0]
  predictedLabel = np.argmax(predictedLabelsProbabilities)
  predictedClassName = CLASSES_LIST[predictedLabel]

  print(f'Action Predicted: {predictedClassName}\nConfidence: {predictedLabelsProbabilities[predictedLabel]}')
  video.release()

print(tf.__version__)
testVideo =  f'/content/drive/MyDrive/KSL/Classes/home/crop_VID20231027182812_home.mp4'
result =  f'/content/drive/MyDrive/KSL/result.mp4'
predictAction(testVideo, SEQUENCE_LENGTH)

def predictVideo(video, result, SEQUENCE_LENGTH):
  video = cv2.VideoCapture(video)
  width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
  height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
  framesQueue = deque(maxlen = SEQUENCE_LENGTH)
  predictedClassName = ''

  while video.isOpened():
    ok, frame = video.read()
    if not ok:
      break
    frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))
    frame = frame / 255.0
    framesQueue.append(frame)
    if len(framesQueue) == SEQUENCE_LENGTH:
      predictedLabelsProbabilities = model.predict(np.expand_dims(framesQueue, axis = 0))[0]
      predictedLabel = np.argmax(predictedLabelsProbabilities)
      predictedClassName = CLASSES_LIST[predictedLabel]
      print(predictedClassName)
      cv2.putText(frame, predictedClassName, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    video.release()
  print(predictedClassName)

predictVideo(testVideo, result, SEQUENCE_LENGTH)